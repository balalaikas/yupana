## Быстрый старт

### Системные требования <a name="requirements"></a>

1. JDK 8;
2. GNU/Linux (работа на других окружениях не проверялась);
3. Apache HBase 1.3.x с поддержкой сжатия Snappy;
4. Apache Spark 2.4.x для запуска запросов на кластере.  Кроме того, в прилагаемых примерах загрузка данных также производится
   из Spark-приложения, хотя это и не является обязательным условием;
5. Кластер Apache Ignite 2.7.0 при использовании распределенных кэшей в Ignite (опционально);
6. sbt -- для сборки проекта.

### Сборка проекта <a name="build"></a>

Сборка проекта осуществляется с помощью sbt.  Некоторые команды в sbt shell:

 - compile -- компиляция проекта
 - test -- запуск юнит-тестов
 - assembly -- сборка толстых jar-ов, применяется в yupana-jdbc и yupana-examples

### Запуск <a name="examples"></a>

Модуль `yupana-examples` содержит пример использования Yupana для анализа транзакций, которые могут быть использованы в
качестве основы при реализации собственных аналитических систем.
В примере реализована схема данных основанная на схеме из пакета yupana-schema с добавлением двух внешних связей (каталог
адресов и каталог организаций).  Каталог адресов (AddressCatalog) использует внутреннюю логику для отображения идентификатора
кассы на город.  Каталог организаций (OrganisationCatalog) отображает кассы на информацию об организации: тип организации
(например аптека, супермаркет) и обезличенный идентификатор. Каталог использует данные из внешнего источника -- базы данных PostgreSQL.

Для запуска примеров необходима база данных PostgreSQL.  По умолчанию используется база данных `yupana-example` на `localhost`.
Базу необходимо создать до запуска примера и миграции:

```
CREATE DATABASE yupana_example;
CREATE USER yupana WITH ENCRYPTED PASSWORD 'yupana';
GRANT CONNECT ON DATABASE yupana_example TO yupana;
```

После создания базы можно мигрировать:

```
sbt examples/flywayMigrate
```

Для запуска миграций с альтернативным адресом сервера PostgreSQL можно использовать команду:

```
sbt -Dflyway.url=jdbc:postgresql://server:port/db_name -Dflyway.user=db_user examples/flywayMigrate
```

#### 1. Server <a name="examples-server"></a>

Реализация сервера на базе yupana-akka.

Запуск из sbt:

```
examples/runMain org.yupana.examples.server.Main
```

Настройки приложения в файле `yupana-examples/src/main/resources/application.conf`.  По умолчанию сервер слушает порт
`10101`.

Для подключения к серверу нужен JDBC драйвер.  Его можно собрать командой `sbt jdbc/assembly`.  Пакет с драйвером будет
сохранен в файл: `yupana/yupana-jdbc/target/scala-2.12/yupana-jdbc-assembly-{версия_проекта}-SNAPSHOT.jar`.
Для соединения с сервером с использованием Yupana JDBC нужно указать следующие параметры: 

  - URL: `jdbc:yupana://localhost:10101`
  - Class name (класс драйвера): `org.yupana.jdbc.YupanaDriver`

#### 2. ETL <a name="examples-etl"></a>

Приложение эмулирует добавление данных Yupana.  Данные генерируются случайным образом.

Для запуска есть скрипт `deploy_etl.sh`. Подразумевается что Apache Spark установлен в `/opt/spark` или задана переменная
окружения `SPARK_HOME`. Перед запуском скрипта необходимо собрать толстый JAR (в sbt `examples/assembly`).

#### 3. QueryRunner <a name="examples-query-runner"></a>

Приложение для запуска запросов к Yupana на кластере Apache Spark.  Результаты сохраняются в виде CSV файла.

SQL запрос для запуска и путь для сохранения результатов задается в `query-runner-app.conf`.

Запуск осуществляется скриптом `deploy_query_runner.sh`

### Адаптация Yupana к существующему окружению <a name="adaptation"></a>

Модуль `yupana-examples` может быть использован в качестве основы для создания собственной аналитической системы.  Для этого потребуется:

1. Определить и реализовать внешние связи для доступа к существующим источникам данных.
2. Определить схему данных на основе существующей схемы.
3. Приведенная в примерах реализация сервера запросов является минимально полной, достаточно использовать схему реализованную
   на шаге 2 схему в сервере.  Однако для интеграции сервера в существующую инфраструктуру скорее всего понадобятся некоторые
   изменения (например чтение настроек из другого источника, использование дополнительных настроек для внешних источников и др).
4. Реализовать ETL процесс для наполнения базы данными.  Для периодического наполнения можно использовать Spark RDD, а для
   потокового DStream.